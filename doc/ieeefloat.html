<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html><head>
    <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
    <meta name="description" content="An overview of IEEE Standard 754 floating-point representation (common to most platforms today).">
    <meta name="keywords" content="floating point, ieee">

    <link rev="made" href="mailto:steve@stevehollasch.com">
    <link rev="made" href="http://stevehollasch.com/"><title>IEEE Standard 754 Floating-Point</title>

    

    <style type="text/css">

        body
        {   
            margin-top:    10%;
            margin-left:   8%;
            margin-right:  8%;
            margin-bottom: 100%;
        }

        h1.title { margin-bottom: 0; }

        h2
        {   margin-top: 2em;
        }

        P
        {   text-indent: 3ex;
            text-align: justify;
            margin-top: 0em;
        }

        P.noindent
        {   text-indent: 0;
            text-align: justify;
            margin-top: 0em;
        }

        li
        {   margin-top: 1em;
        }

        .indent
        {   margin-left: 4em;
        }

        .byline
        {   font-size: smaller;
        }
    </style></head>

<body>

<h1 class="title"><nobr>IEEE Standard 754</nobr>
    <nobr>Floating Point Numbers</nobr></h1>

<span class="byline">
    <a href="mailto:steve@stevehollasch.com?Subject=IEEE%20Floating%20Point%20Page">
    Steve Hollasch</a>
    / Last update 2005-Feb-24
</span>

<hr style="margin-bottom: 3em;">


<p class="noindent">
IEEE Standard 754 floating point is the most common representation today for
real numbers on computers, including Intel-based PC's, Macintoshes, and most
Unix platforms. This article gives a brief overview of IEEE floating point and
its representation. Discussion of arithmetic implementation may be found in the
book mentioned at the bottom of this article.


</p><h2 id="intro">What Are Floating Point Numbers?</h2> 

<p class="noindent">
There are several ways to represent real numbers on computers. Fixed point
places a radix point somewhere in the middle of the digits, and is equivalent
to using integers that represent portions of some unit. For example, one might
represent 1/100ths of a unit; if you have four decimal digits, you could
represent 10.82, or 00.01. Another approach is to use rationals, and represent
every number as the ratio of two integers.

</p><p>
Floating-point representation - the most common solution - basically represents
reals in scientific notation. Scientific notation represents numbers as a base
number and an exponent. For example, 123.456 could be represented as 1.23456
&#215; 10<sup>2</sup>. In hexadecimal, the number 123.abc might be represented
as 1.23abc &#215; 16<sup>2</sup>.

</p><p>
Floating-point solves a number of representation problems. Fixed-point has a
fixed window of representation, which limits it from representing very large or
very small numbers. Also, fixed-point is prone to a loss of precision when two
large numbers are divided.

</p><p>
Floating-point, on the other hand, employs a sort of "sliding window" of
precision appropriate to the scale of the number. This allows it to represent
numbers from 1,000,000,000,000 to 0.0000000000000001 with ease.

</p><h2 id="storage">Storage Layout</h2>

<p class="noindent">
IEEE floating point numbers have three basic components:  the sign, the
exponent, and the mantissa. The mantissa is composed of the <dfn>fraction</dfn>
and an implicit leading digit (explained below). The exponent base (2) is
implicit and need not be stored.

</p><p>
The following figure shows the layout for single (32-bit) and  double (64-bit)
precision floating-point values. The number of bits for each field are shown
(bit ranges are in square brackets):

<br>
<br>
</p><div align="center">
<table border="2" cellpadding="4">
<tbody><tr><th></th>
    <th>Sign</th>
    <th>Exponent</th>
    <th>Fraction</th>
    <th>Bias</th>
</tr>

<tr><th>Single Precision</th>
    <td align="center">1 [31]</td>
    <td align="center">8 [30-23]</td>
    <td align="center">23 [22-00]</td>
    <td align="center">127</td>
    </tr>

<tr><th>Double Precision</th>
    <td align="center">1 [63]</td>
    <td align="center">11 [62-52]</td>
    <td align="center">52 [51-00]</td>
    <td align="center">1023</td>
    </tr>
</tbody></table>
</div>


<h3>The Sign Bit</h3>

<p class="noindent">
The sign bit is as simple as it gets. 0 denotes a positive number; 1 denotes a
negative number. Flipping the value of this bit flips the sign of the number.

</p><h3>The Exponent</h3>

<p class="noindent">
The exponent field needs to represent both positive and negative exponents.  To
do this, a <dfn>bias</dfn> is added to the actual exponent in order to get the
stored exponent. For IEEE single-precision floats, this value is 127.  Thus, an
exponent of zero means that 127 is stored in the exponent field.  A stored
value of 200 indicates an exponent of (200-127), or 73. For reasons discussed
later, exponents of -127 (all 0s) and +128 (all 1s) are reserved for special
numbers.

</p><p>
For double precision, the exponent field is 11 bits, and has a bias of 1023.

</p><h3>The Mantissa</h3>

<p class="noindent">
The <dfn>mantissa</dfn>, also known as the <dfn>significand</dfn>, represents
the precision bits of the number. It is composed of an implicit leading bit and
the fraction bits.

</p><p>
To find out the value of the implicit leading bit, consider that any number can
be expressed in scientific notation in many different ways. For example, the
number five can be represented as any of these:

</p><p></p><pre>        5.00 &#215; 10<sup>0</sup>
        0.05 &#215; 10<sup>2</sup>
        5000 &#215; 10<sup>-3</sup>
</pre>

<p class="noindent">
In order to maximize the quantity of representable numbers, floating-point
numbers are typically stored in <dfn>normalized</dfn> form. This basically puts
the radix point after the first non-zero digit. In normalized form, five is
represented as 5.0 &#215; 10<sup>0</sup>.

</p><p>
A nice little optimization is available to us in base two, since the only
possible non-zero digit is 1. Thus, we can just assume a leading digit of 1,
and don't need to represent it explicitly. As a result, the mantissa has
effectively 24 bits of resolution, by way of 23 fraction bits.


</p><h3>Putting it All Together</h3>
<p class="noindent">
So, to sum up:

</p><p></p><ol class="indent">
    <li>The sign bit is 0 for positive, 1 for negative.
    </li><li>The exponent's base is two.
    </li><li>The exponent field contains 127 plus the true exponent for
        single-precision, or 1023 plus the true exponent for double precision.
    </li><li>The first bit of the mantissa is typically assumed to be 1.<i>f</i>,
        where <i>f</i> is the field of fraction bits.
</li></ol>

<h2 id="ranges">Ranges of Floating-Point Numbers</h2>

<p class="noindent">
Let's consider single-precision floats for a second. Note that we're taking
essentially a 32-bit number and re-jiggering the fields to cover a much broader
range. Something has to give, and it's precision. For example, regular 32-bit
integers, with all precision centered around zero, can precisely store integers
with 32-bits of resolution. Single-precision floating-point, on the other hand,
is unable to match this resolution with its 24 bits. It does, however,
approximate this value by effectively truncating from the lower end. For
example:

</p><p></p><pre>      11110000 11001100 10101010 00001111  // 32-bit integer
  = +1.1110000 11001100 10101010 x 2<sup>31</sup>     // Single-Precision Float
  =   11110000 11001100 10101010 00000000  // Corresponding Value
</pre>

<p class="noindent">
This approximates the 32-bit value, but doesn't yield an exact representation.
On the other hand, besides the ability to represent fractional components
(which integers lack completely), the floating-point value can represent
numbers around 2<sup>127</sup>, compared to 32-bit integers maximum value
around 2<sup>32</sup>.

</p><p>
The range of positive floating point numbers can be split into normalized
numbers (which preserve the full precision of the mantissa), and
<i>denormalized</i> numbers (discussed later) which use only a portion of the
fractions's precision.

</p><p>
</p><center>
<table border="1" cellpadding="6">

    <tbody><tr><th></th>
        <th>Denormalized</th>
        <th>Normalized</th>
        <th>Approximate Decimal</th>
    </tr><tr><th>Single Precision</th>
        <td><nobr>&#177; 2<sup>-149</sup> to
                  (1-2<sup>-23</sup>)&#215;2<sup>-126</sup></nobr></td>
        <td><nobr>&#177; 2<sup>-126</sup> to
                  (2-2<sup>-23</sup>)&#215;2<sup>127<sup></sup></sup></nobr></td>
        <td><nobr>&#177; ~10<sup>-44.85</sup> to
                  ~10<sup>38.53</sup></nobr></td>
    </tr><tr><th>Double Precision</th>
        <td><nobr>&#177; 2<sup>-1074</sup> to
                  (1-2<sup>-52</sup>)&#215;2<sup>-1022</sup></nobr></td>
        <td><nobr>&#177; 2<sup>-1022</sup> to
                  (2-2<sup>-52</sup>)&#215;2<sup>1023<sup></sup></sup></nobr></td>
        <td><nobr>&#177; ~10<sup>-323.3</sup> to
                  ~10<sup>308.3</sup></nobr></td>

</tr></tbody></table>
</center>
<br>

<p>
Since the sign of floating point numbers is given by a special leading bit,
the range for negative numbers is given by the negation of the above values.

</p><p>
There are five distinct numerical ranges that single-precision floating-point
numbers are <strong>not</strong> able to represent:

</p><p></p><ol>
    <li>Negative numbers less than -(2-2<sup>-23</sup>) &#215; 2<sup>127</sup>
        (<dfn>negative overflow</dfn>)
    </li><li>Negative numbers greater than -2<sup>-149</sup>
        (<dfn>negative underflow</dfn>)
    </li><li>Zero
    </li><li>Positive numbers less than 2<sup>-149</sup>
        (<dfn>positive underflow</dfn>)
    </li><li>Positive numbers greater than (2-2<sup>-23</sup>)
        &#215; 2<sup>127</sup>
        (<dfn>positive overflow</dfn>)
</li></ol>

<p>
Overflow means that values have grown too large for the representation, much
in the same way that you can overflow integers. Underflow is a less serious
problem because is just denotes a loss of precision, which is guaranteed to
be closely approximated by zero.

</p><p>
Here's a table of the effective range (excluding infinite values) of IEEE
floating-point numbers:

</p><p>
</p><center>
<table border="1" cellpadding="6">

<tbody><tr><th></th>
    <th>Binary</th>
    <th>Decimal</th>
    </tr>

<tr><th>Single</th>
    <td>&#177; (2-2<sup>-23</sup>) &#215; 2<sup>127</sup></td>
    <td>~ &#177; 10<sup>38.53</sup></td>

</tr><tr><th>Double</th>
    <td>&#177; (2-2<sup>-52</sup>) &#215; 2<sup>1023</sup></td>
    <td>~ &#177; 10<sup>308.25</sup></td>

</tr></tbody></table>
</center>

<p><i>
Note that the extreme values occur (regardless of sign) when the exponent is at
the maximum value for finite numbers (2<sup>127</sup> for single-precision,
2<sup>1023</sup> for double), and the mantissa is filled with 1s (including
the normalizing 1 bit).
</i>



</p><h2 id="specials">Special Values</h2>

<p class="noindent">
IEEE reserves exponent field values of all 0s and all 1s to denote
special values in the floating-point scheme.

</p><h3 id="zero">Zero</h3>
<p class="noindent">
As mentioned above, zero is not directly representable in the straight format,
due to the assumption of a leading 1 (we'd need to specify a true zero
mantissa to yield a value of zero). Zero is a special value denoted with an
exponent field of zero and a fraction field of zero. Note that -0 and +0 are
distinct values, though they both compare as equal.

</p><h3 id="denormalized">Denormalized</h3>
<p class="noindent">
If the exponent is all 0s, but the fraction is non-zero (else it would be
interpreted as zero), then the value is a <dfn>denormalized</dfn> number, which
does <em>not</em> have an assumed leading 1 before the binary point. Thus, this
represents a number <nobr>(-1)<sup><i>s</i></sup> &#215; 0.<i>f</i> &#215;
2<sup>-126</sup></nobr>, where <i>s</i> is the sign bit and <i>f</i> is the
fraction. For double precision, denormalized numbers are of the form
<nobr>(-1)<sup><i>s</i></sup> &#215; 0.<i>f</i> &#215;
2<sup>-1022</sup></nobr>.  From this you can interpret zero as a special type
of denormalized number.

</p><h3 id="infinity">Infinity</h3>
<p class="noindent">
The values +infinity and -infinity are denoted with an exponent of all 1s
and a fraction of all 0s. The sign bit distinguishes between negative
infinity and positive infinity. Being able to denote infinity as a specific
value is useful because it allows operations to continue past overflow
situations. <em>Operations with infinite values are well defined in
IEEE floating point.</em>

</p><h3 id="nan">Not A Number</h3>
<p class="noindent">
The value <acronym>NaN</acronym> (<dfn>Not a Number</dfn>) is used
to represent a value that does not represent a real number. NaN's are
represented by a bit pattern with an exponent of all 1s and a non-zero
fraction. There are two categories of NaN:  <acronym>QNaN</acronym>
(<dfn>Quiet NaN</dfn>) and <acronym>SNaN</acronym>
(<dfn>Signalling NaN</dfn>).

</p><p>
A QNaN is a NaN with the most significant fraction bit set. QNaN's propagate
freely through most arithmetic operations. These values pop out of an
operation when the result is not mathematically defined.

</p><p>
An SNaN is a NaN with the most significant fraction bit clear. It is used to
signal an exception when used in operations. SNaN's can be handy to assign
to uninitialized variables to trap premature usage.

</p><p>
Semantically, QNaN's denote <em>indeterminate</em> operations, while SNaN's
denote <em>invalid</em> operations.



</p><h2 id="operations">Special Operations</h2>


<p class="noindent">
Operations on special numbers are well-defined by IEEE. In the simplest case,
any operation with a NaN yields a NaN result. Other operations are as follows:

</p><p>
</p><center>
<table border="1" cellpadding="4">
    <tbody><tr><th align="center">Operation</th><th align="center">Result
    </th></tr><tr><td align="center">n &#247; &#177;Infinity
        </td><td align="center">0
    </td></tr><tr><td align="center">&#177;Infinity &#215; &#177;Infinity
        </td><td align="center">&#177;Infinity
    </td></tr><tr><td align="center">&#177;nonzero &#247; 0
        </td><td align="center">&#177;Infinity
    </td></tr><tr><td align="center">Infinity + Infinity
        </td><td align="center">Infinity
    </td></tr><tr><td align="center">&#177;0 &#247; &#177;0
        </td><td align="center"><i>NaN</i>
    </td></tr><tr><td align="center">Infinity - Infinity
        </td><td align="center"><i>NaN</i>
    </td></tr><tr><td align="center">&#177;Infinity &#247; &#177;Infinity
        </td><td align="center"><i>NaN</i>
    </td></tr><tr><td align="center">&#177;Infinity &#215; 0
        </td><td align="center"><i>NaN</i>
</td></tr></tbody></table>
</center>

<h2 id="summary">Summary</h2>


<p class="noindent">
To sum up, the following are the corresponding values for a given
representation:

</p><p>
</p><div align="center">
<table border="2" cellpadding="4">

<caption>Float Values (<i>b</i> = bias)</caption>

<tbody><tr><th>Sign</th><th>Exponent (<i>e</i>)</th><th>Fraction (<i>f</i>)</th><th>Value

</th></tr><tr><td align="center">0
    </td><td align="center">00..00
    </td><td align="center">00..00
    </td><td align="center">+0

</td></tr><tr><td align="center">0
    </td><td align="center">00..00
    </td><td align="center">00..01<br>:<br>11..11
    </td><td align="center">Positive Denormalized Real<br>
                     0.<i>f</i> &#215; 2<sup>(-<i>b</i>+1)</sup>

</td></tr><tr><td align="center">0
    </td><td align="center">00..01<br>:<br>11..10
    </td><td align="center">XX..XX
    </td><td align="center">Positive Normalized Real<br>
                     1.<i>f</i> &#215; 2<sup>(<i>e</i>-<i>b</i>)</sup>

</td></tr><tr><td align="center">0
    </td><td align="center">11..11
    </td><td align="center">00..00
    </td><td align="center">+Infinity

</td></tr><tr><td align="center">0
    </td><td align="center">11..11
    </td><td align="center">00..01<br>:<br>01..11
    </td><td align="center">SNaN

</td></tr><tr><td align="center">0
    </td><td align="center">11..11
    </td><td align="center">10..00<br>:<br>11..11
    </td><td align="center">QNaN

</td></tr><tr><td align="center">1
    </td><td align="center">00..00
    </td><td align="center">00..00
    </td><td align="center">-0

</td></tr><tr><td align="center">1
    </td><td align="center">00..00
    </td><td align="center">00..01<br>:<br>11..11
    </td><td align="center">Negative Denormalized Real<br>
                     -0.<i>f</i> &#215; 2<sup>(-<i>b</i>+1)</sup>

</td></tr><tr><td align="center">1
    </td><td align="center">00..01<br>:<br>11..10
    </td><td align="center">XX..XX
    </td><td align="center">Negative Normalized Real<br>
                     -1.<i>f</i> &#215; 2<sup>(<i>e</i>-<i>b</i>)</sup>

</td></tr><tr><td align="center">1
    </td><td align="center">11..11
    </td><td align="center">00..00
    </td><td align="center">-Infinity

</td></tr><tr><td align="center">1
    </td><td align="center">11..11
    </td><td align="center">00..01<br>:<br>01..11
    </td><td align="center">SNaN

</td></tr><tr><td align="center">1
    </td><td align="center">11..11
    </td><td align="center">10..00<br>:<br>11.11
    </td><td align="center">QNaN

</td></tr></tbody></table>
</div>


<h2 id="refs">References</h2>

<p class="noindent">
A lot of this stuff was observed from small programs I wrote to go back and
forth between hex and floating point (<i>printf</i>-style), and to examine the
results of various operations. The bulk of this material, however, was lifted 
from Stallings' book.

</p><ol>

<li>
<cite>Computer Organization and Architecture</cite>,
William Stallings, pp. 222-234
Macmillan Publishing Company,
ISBN 0-02-415480-6

</li><li>
IEEE Computer Society (1985),
<cite>IEEE Standard for Binary Floating-Point Arithmetic</cite>,
IEEE Std 754-1985. 

</li><li>
<cite>
Intel Architecture Software Developer's Manual, Volume 1: Basic Architecture
</cite>, (a PDF document downloaded from <a href="http://intel.com/">intel.com</a>.)


</li></ol>



<h2 id="seealso">See Also</h2>

<ul>

<li><a href="http://standards.ieee.org/">IEEE Standards Site</a>

</li><li>
<cite>Comparing floating point numbers</cite>, Bruce Dawson,
<a href="http://www.cygnus-software.com/papers/comparingfloats/comparingfloats.htm">
http://www.cygnus-software.com/papers/comparingfloats/comparingfloats.htm</a>.
This is an excellent article on the traps, pitfalls and solutions for comparing
floating point numbers. Hint &#8212; epsilon comparison is usually the
<em>wrong</em>
solution.

</li><li>
<cite>x86 Processors and Infinity</cite>, Bruce Dawson,
<a href="http://www.cygnus-software.com/papers/x86andinfinity.html">
http://www.cygnus-software.com/papers/x86andinfinity.html</a>.  This is another
good article covering performance issues with IEEE specials on X86
architecture.

</li></ul>

<div style="text-align: right; font-size: smaller; margin-top: 3em;">
    <hr>
    &#169; 2001-2005
    <a href="mailto:steve@stevehollasch.com?Subject=IEEE%20Floating%20Point%20Page">
    Steve Hollasch</a>
</div>

</body></html>